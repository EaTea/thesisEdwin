%-----------------------------------------------------------------------------%
%Packages%
\documentclass[12pt, a4paper, titlepage]{scrartcl}
\usepackage{amsmath, amsfonts, listings, amssymb, mathtools, amsthm} %Mathematical Expressions package
\usepackage{mathtools}
\usepackage[usenames, dvipsnames]{color} %Color naming packages
\usepackage[margin=1.5cm]{geometry}
\usepackage{float}
\usepackage{verbatim} %for code
\usepackage[pdftex]{graphics}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{tikz}
\usepackage{comment}
\usepackage[nottoc]{tocbibind}
\usepackage[square]{natbib}
\usepackage{caption}
\usepackage{subcaption}

\addtokomafont{disposition}{\rmfamily}
\usetikzlibrary{arrows,shapes}

%Graphis Extensions
\DeclareGraphicsExtensions{.png, .jpg}
\parindent 0pt

% Predefined things such as commands, etc.

\newcommand{\aRel}[1] {
  \sim_{#1} 
}

\newcommand{\kripkeFrame}[2] {
  (#1, \aRel{#2})
}

\newcommand{\kripkeModel}[3] {
  (#1, \aRel{#2}, #3)
}

\newcommand{\frKripModel}[2] { % defined via Kripke Frame + valuation
  (#1, #2)
}

\newcommand{\actModel}[3]{
  (#1, \aRel{#2}, #3)
}

\newcommand{\frActModel}[2] { % defined via Kripke Frame + Pre
  (#1, #2)
}

\newcommand{\note}[1]{\textsc{\textbf{#1}}}
\newcommand{\Universal}{$\mathcal{U}$}
\newcommand{\modalLog}{$\mathcal{L}$}
\newcommand{\modLogInf}{$\mathcal{L}_\inf$}
\newcommand{\epActLog}{\modalLog$([\alpha])$}
\newcommand{\epActLogCommonKnowledge}{\modalLog$([\alpha],\box^{*})$}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{remrk}{Remark}

% Drawings of frames

\tikzstyle{vertex}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
\tikzstyle{selected vertex} = [vertex, fill=red!24]
\tikzstyle{edge} = [draw,thick,->]
\tikzstyle{weight} = [font=\small]

%-----------------------------------------------------------------------------%
%Document%
\begin{document}

\title{A Survey of Dynamic Epistemic Logic and Action Models}
\subtitle{UWA Final Year Engineering Project Literature Review}
\author{Edwin Tay, 20529864}
\date{\today}

\maketitle

\pagebreak

\tableofcontents

\vfill
\pagebreak

\section{Introduction}\label{intro}
\subsection{Describing Information State}\label{intro_infoState}
Multi-agent system are found in many fields, including communications systems,
artificial intelligence and games.
In part, Multi-agent Dynamic Epistemic Logic is concerned with modelling and reasoning
about what agents in these systems know.
This extends to how models are updated and what we can reason about agents'
perceptions of actions in a multi-agent system.
Being able to logically reason about what agents know and what actions they perceive
allows us to construct verifiable and correct specifications and claims about
models of situations.\\
\\
The change of agent knowledge via informative updates is a field that has seen
much attention recently.
Theorists have constructed models that capture these informative updates with
increasing expressiveness.
We discuss the increasing power and specificity of these models throughout this
survey, with a particular interest in Public Announcement Logic in Section
\ref{pal} and Epistemic Actions and Action Models in Section \ref{estAct}. \\
\\
In a similar manner to models of knowledge, models of information updates can be
used to formally reason about updates.
The rigour of formalism is appealing for an application that aims to guarantee
security or verify that it fulfills a specification.
This formalism to formal, provable methods of software development,
communication protocol verification as well as economics and game theory.\\
\\
Recent work has focused on the synthesis of informative updates, showing how an
informative update can be constructed to ensure that, if possible, a multi-agent system
satisfies a condition after an update. \cite{hales13synthesis}
However, translating a formal model describing an informative update into an
implementation of communication between multiple agents is an unexplored
process.
Transforming a formally correct and verifiable specification of
communication into an executable protocol within a real world system would have
wide implications for software engineering, security and safety in
mission-critical systems with high-correctness requirements.
\subsection{Coin-Flipping Game}\label{intro_coinFlipping}
Consider a game being played between two friends (or agents), Angeline $(A)$
and Ben $(B)$.
The game involves flipping a coin, hiding the result from both $A$ and $B$ and
having both of them guess whether the coin is Heads $(H)$ or the coin is Tails
$(T)$.
The game is refereed by a mutually trusted (and usually impartial) friend, Carol
$(C)$ who knows if the coin is $H$ or $T$.\\
\\
$A$ and $B$ know that either $H$ is true or $T$ is true.
They also know that $H$ and $T$ cannot both occur simultaneously together.
An agent's knowledge (which is inclusive of, but not limited to the previous
statements) is called the Epistemic State.
Furthermore, there are two possibilities for the coin's outcome: that it is $H$
or $T$.
Neither $A$ or $B$ as they are can distinguish between them; that is, they are
uncertain which of the outcomes is true.
Agent uncertainty in this multi-agent system is another aspect we must attempt
to capture.\\
\\
We will concern ourselves specifically with models whose facts cannot change,
and updates that thus do not contradict previously established facts.
This literature review considers the relevant writings and logics to describe
the Epistemic State and how we can change and informatively update the Epistemic
state.
The following sections will explore how we can model static situations and
different informative updates.
We will note what has not been explored in the fields and compare frameworks'
differing approaches and strengths.

\section{Epistemic Modal Logic}\label{Epistemic}
In the coin-flipping game, we can make the following observations.
\begin{itemize}
	\item $A$ considers $H$ to be possible, as well as $T$ to be possible
	\item $B$ also considers $H$ to be possible, and also considers $T$ to be possible
\end{itemize}
We claim that there are two possible worlds that differ in one way ---
in one of these worlds, $H$ is true, and in the other world $T$ is true.
Furthermore, these worlds are the same from $A$ and $B$'s perspectives.
$A$ would not be able to distinguish the world where $H$ was true from either of
the world where $H$ was true or the world where $T$ was true, since she cannot
see the coin.\\
\\
We can represent this uncertainty between what world is true as relations
between worlds.
In this case, let $W = \{ \eta, \tau\}$ be our set of possible worlds, where $H$ is true
at $\eta$ and $T$ is true at $\tau$.
Let $R_A = R_B = \{(\tau,\tau), (\tau,\eta), (\eta, \tau), (\eta,\eta)\}$ be
binary relations on $W$ representing indistinguishability between two possible
worlds.
We call these relations ``accessibility relations"; we say if one world is
indistinguishable from another, then they can access each other.
Lastly, let $V$ be a valuation function that maps a formula to the set of worlds
where that formula is true, so $V(H) = \{\eta\}$ and $V(T) = \{\tau\}$.
\citep{hoek2008dynamic, blackburn2002modal}
\begin{defn}
	The tuple
	\[
		M = (W, R = R_A \cup R_B, V)
	\]
	is an Epistemic model of our game of heads and tails.
\end{defn}

We can represent this model graphically as shown in Figure \ref{htkripkefigure},
where our possible worlds $(W)$ are nodes and our binary relations $(R$) are edges in the
graph.
\begin{figure}[ht!]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
      thick]

    \node[vertex] (1) {$\eta$};
    \node[vertex] (2) [right of=1] {$\tau$};
    \path[edge]
          (1) edge node {$A,B$} (2)
              edge [loop left] node {$A,B$} (1)
          (2) edge node {} (1)
              edge [loop right] node {$A,B$} (2);
\end{tikzpicture}
\caption{A graph representation of our game of heads and tails.}\label{htkripkefigure}
\end{figure}

The possible worlds models form a semantics allowing us to construct meaningful
models that reflect an Epistemic State.
These models allow us to reason about relations between the possible worlds in
an internal sense, from $A$ and $B$'s perspectives. \citep{blackburn2002modal}
We can define an operator to formally represent the modality of ``knowing"
a proposition or formula to be true.
\begin{defn}
	The operator $\Box_A H$ stands for ``$A$ knows $H$ is true".
	$\Box_A \phi$ is true at a possible world $w$ if for all accessibility relations $(w,
	w')$, $\phi$ is true.
\end{defn}
We also define the dual operator of knowing, which is to consider something
possible.
\begin{defn}
	The operator $\diamond_B T$ stands for ``$B$ considers $T$ to be possible".
	$\diamond_B \phi$ is true at a possible world $w$ if there is one
	accessibility relation $(w,w')$ such that $\phi$ is true at $w'$.\\
	
	\begin{note}
		N.B.: Notice that we can define $\diamond$ in terms of $\Box$, where $\diamond
		\phi \iff \neg \Box \neg \phi$.
	\end{note}
\end{defn}
We will omit the name of the agent for operators $\Box$ and $\diamond$ when we
discuss a single-agent case, as follows.
We now show, without proof, four axioms that must hold for our modality of
knowledge.
\begin{thm}
	The following are true at any world of an Epistemic Model.
	\begin{enumerate}
		\item $\Box (\phi \implies \theta) \implies (\Box \phi \implies \Box
				\theta)$, known as {\bf K}
		\item $\Box \phi \implies \phi$, known as {\bf T}
		\item $\Box \phi \implies \Box \Box \phi$, labelled {\bf 4}
		\item $\neg \Box \phi \implies \Box (\neq \Box \phi))$, known as {\bf 5}
	\end{enumerate}
	\citep{hoek2008dynamic}
\end{thm}

\begin{note}
In order to satisfy these conditions, binary relations between possible worlds
in our Epistemic Models become equivalence relations.\\
\\
Also, if we replaced {\bf T} with an axiom {\bf D} which states $\Box \phi
\implies \diamond \phi$, then we can model Doxastic Models of belief, instead of
Epistemic Models of knowledge.
\end{note}
We can now make formal reasonings, such as deciding that $\Box_A H$ and $\Box_A
T$ are both false.
We note that $\Box_B (H \lor T)$ is true and $\Box_A \Box_B (H \lor T)$ is
true.\\
\\
{\bf K}, {\bf T}, {\bf 4} and {\bf 5} allow us to model Epistemic State and
uncertainty in our game of heads and tails.
We have defined on our models that allows us to make formal reasonings and
decide if formulae are true at possible worlds.\\
\\
But what if their friend, $C$, announces ``The coin is Heads up" (that is,
$C$ announces $H$)?
As they are, our models cannot describe the change in knowledge that
$C$'s announcement will entail.
Furthermore, we have no formal process to say how the situation has changed.\\
\\
This deficiency raises the following questions:
\begin{itemize}
	\item How do we describe this change in a formal manner?
	\item What reasoning can we make about the state of information after this
	change?
	\item Is there an operation that allows us to change the state of information
	from the pre-change state to the post-change state?
\end{itemize}

\section{Public Announcement Logic}\label{pal}
To model the announcement of facts and information, we turn to the use of public
announcement logic.
Public announcements are announcements to multiple agents of facts.
They are a kind of informative update --- perhaps the most basic kind.\\
\\
Public announcement logic was first proposed independently by both Plaza and
Gerbrandy and Groeneveld. \citep{plaza2007public,gelbrandy1997reasoning}
They were later augmented by the work of Baltag, Moss and Solecki through the
addition of common knowledge. \citep{baltag1998lpa}
Public announcements are informational updates of true facts that change the
knowledge state amongst multiple agents.\\
\\
Public announcement of a fact $\phi$ cause all worlds where $\phi$ was
invalid to ``disappear" from our model.
It is thus quite easy to model the execution of a public announcements.
The execution simply causes all worlds that are no longer consistent with the
announcement to ``disappear" from our model of a given situation.\\
\\
Public announcements capture changes in information, such as $C$ saying to
$A$ and $B$ that the coin is $H$.
Another possible update is $A$ being allowed to see the coin's state, and
telling $B$ ``I guess you didn't know that the coin is actually heads up".\\
\\
These changes can be successful or unsuccessful, depending on whether the fact
that was announced is true after its announcement.
For example, if $A$ makes the (truthful) announcement that $H$, then this
announcement is successful since $H$ will be true after the announcement.\\
\\
Conversely, an announcement from $A$ that ``I know that you {\em don't} know $H$
and the coin is $H$" will be false after the announcement, since $A$'s
announcement has changed the knowledge state to invalidate her
announcement.
Therefore, the second example is an unsuccessful update.\\
\\
By using public announcement logic we can describe these announcements of
information.
We have a framework to describe and change the state of information amongst our
agents before and after announcements of facts.
Public announcement logic will allow us to model $C$ telling $A$ and $B$ in a
public fashion that $H$ is true.\\
\\
In terms of dynamic epistemic logic, however, there could be many more kinds of
updates that we have yet to consider.
Re-examining our game of heads and tails yields scenarios that we cannot
describe:
\begin{itemize}
	\item What about $A$ cheating and learning $H$ or $T$ without $B$'s knowledge?
	\item What about $B$ beginning to suspect $A$ of cheating?
	\item What if $C$ was to whisper (in front of $B$) whether the coin was $H$
	or $T$?
\end{itemize}
These are things public announcement logic cannot describe, and our inability to
model them raises more questions:
\begin{itemize}
	\item what other kinds of epistemic (or informative) updates exist?
	\item how can we describe other epistemic updates in a sensible manner?
	\item what kind of an execution is required for a more nontrivial update?
\end{itemize}

\section{Epistemic Actions and Action Models} \label{estAct}
We can now successfully capture the simple act of announcing a fact.
However, in public announcements, we cannot capture certain epistemic actions,
such as
\begin{itemize} 
  \item $C$ whispers to $A$ that $H$ is true and $B$ sees her whisper
  \item $C$ whispers to $A$ that $H$ is true without $B$ seeing
  \item $B$ suspects $A$ of cheating, but he isn't sure if she's cheated
\end{itemize}
Let us aim to generalise the ideas behind public announcements to capture other
informative updates.\\
\\
We will now constrain our agents' behaviours to only being able to accept facts.
They will not worry about changing facts, and if facts do change we will take it
as something that causes our agents knowledge systems to simply crash.\\
\\
We will examine two contrasting approaches to model these epistemic updates.
First, we examine the approach of van Ditsmarsch in constructing epistemic
actions.
\subsection{Epistemic Relational Actions} \label{epi_acts}
van Ditsmarsch approaches epistemic actions from a syntactical point of view,
aiming to create a language to specify actions in.
His work in constructing an epistemic action syntax draws from some of the
syntax of established languages such as propositional dynamic
logic.\citep{ditmarsch99knowledge,ditmarsch2002dga}\\
\\
We will label his language as $\mathcal{L}_{!}$.
Within $\mathcal{L}_{!}$ van Ditsmarsch provides operations to make valid
epistemic statements, and further extends the language with dynamic or
action-oriented constructs.
These include the ability to test a proposition, to update a group of agents'
knowledge and to make a non-deterministic choice between actions.\\
\\
$\mathcal{L}_{!}$ presents a way to describe complex actions, and indeed we can
express and describe all the actions we've discussed in the previous sections.
We can formally describe actions such as cheating, or a private
announcement to $B$ that $H$ is true that's seen by $A$.\\
\\
However, reasoning about epistemic actions in their current form is difficult.
Indeed, the interpretations of an epistemic action are non-trivial to
understand.
Moreover, van Ditsmarsch, van der Hoek and Kooi present problems with
representing complex uncertainties with $\mathcal{L}_{!}$ that make it difficult
to reason about what $A$ and $B$ know after an action takes place.\\
\\
As an example, given two actions where a choice between actions $\Omega$ and
$\Gamma$ occurs, in which $B$ learns that $A$ learns either of $p \land q$ or
$p \land \neg q$ respectively.
We can see that $B$ should know that $A$ knows $p$ after either of $\Omega$ or
$\Gamma$ occurring.
In order to recognise this, we need a generalisation of our rules on what is
true after an action is executed.
Unfortunately, to generalise this principle and allow us to reason this out is
not provided in $\mathcal{L}_{!}$.\citep{hoek2008dynamic}\\
\\
This motivates our exploration of the work of Baltag, Moss and Solecki, who construct
another framework to model epistemic updates.
van Ditsmarsch, van der Hoek and Kooi note that this framework naturally
overcomes this problem by having the idea of accessibility between action
outcomes encoded into the model itself.
\subsection{Epistemic Action Models} \label{act_mods}
As an alternative to approaching epistemic actions syntactically, Baltag, Moss
and Solecki examine epistemic actions from a modelling point of view.
They aim to construct epistemic Action Models, which resemble the possible
worlds models of Kripke semantics. \citep{baltag1998lpa}
\footnote{Action Models resemble the possible worlds models of Kripke semantics,
but there were some discrepancies which meant they were not entirely equivalent.
Aucher resolves this and restores the symmetry between the models, allowing us
to reason about the actions within Action Models. \citep{aucher09revisited}}
This ``possible actions" model addresses the problems that van Ditsmarsch, van
der Hoek and Kooi highlighted with epistemic actions.\\
\\
Baltag and Moss extend the Action Models they proposed with Solecki into classes
of Action Models, which they term action signatures. \citep{baltag2005programs}
They formalise general descriptions of actions, such as announcements, lying,
suspicion and other epistemic updates.
As an example, they generalise all public announcements to a single structure,
showing the superior expressiveness of Action Models.
They also specify a syntax for discussing actions of a given action signature,
allowing them to generate the logic of public announcements in its entirety.
Within this syntax, the Action Model structures are used as syntactical objects.
Like van Ditsmarsch's Epistemic Actions, Baltag and Moss' syntax borrows from
previous logics of dynamics and change.\\
\\
Action Models, being updates in themselves, are also define a method of
execution that is well-understood.
The execution will update a model of knowledge from a pre-update to a
post-update state.
Unfortunately, the cost of action model execution (that is, to transform a state
of information to a new state using an update specified by an action model) is
quite high.
It involves taking a Cartesian product of all possible actions.
This indicates that, given $N$ possible actions in an Action Model, $N^2$
actions must be computed in the new state of information before the model can
be refined.

\subsubsection{Comparison to Relational Epistemic Actions} \label{epi_compare}
Both Relational Epistemic Actions and Action Models describe a larger range of
epistemic updates for our game of heads and tails.
The examples we chose earlier, regarding $C$ telling $A$ about the state of the
coin, or $B$ becoming suspicious of $A$ can be expressed using either framework.
What, then, is the real difference between employing Action Models, as opposed
to Relational Epistemic Actions?\\
\\
van Ditsmarsch, van der Hoek and Kooi, as well as Baltag and Moss independently
note that the ``possible actions" of Action Models  are actually relational
epistemic actions.
In their own review of Action Models, van Ditsmarsch, van der Hoek and Kooi
motivate this with an example. \citep{hoek2008dynamic,baltag2005programs}\\
\\
They consider the non-deterministic choice of 3 possible actions which are
indistinguishable externally.
As an example, $B$ might learn that $A$ knows either of $H$ or $T$, or has
learnt nothing at all.
Since $B$ does not know which of these actions could occur, each of these
epistemic updates is a possible action.\\
\\
They show that it is possible to express this uncertainty in a manner quite similar
to an Action Model.
Similarly, the corresponding Action Model of whether $B$ learns of $A$'s
knowledge of $H$, $T$ or nothing new is quite easily interpreted in a manner
akin to Relational Epistemic Actions.
Their general result is that relational actions can be expressed as action
models, and vice versa.
As van Ditsmarsch, van der Hoek and Kooi have already shown, it is in reasoning
with these models that the models differ.\\
\\
Notably, Relational Epistemic Actions and Action Models suffer from the same
weakness, in that they are only externally describe an update of information.
Their execution and specification are only useful to a third-party viewer, and
how agents internal to a system update their knowledge is not clear.
This is one of several weaknesses in current dynamic epistemic logic, with
regards to translating a specification of an informational update into an
implementable series of messages.\\
\\
It is in their reasoning that Action Models and Relational Epistemic Actions
differ most.
Action Models give us a notion of uncertainty between actions in a manner
similar to Kripke semantics, allowing us to use a formally established framework
to reason about dynamic updates.
This allows us to reason about our updates and their effects in a more natural
and well-understood way.\\
\\
Conversely, van Ditsmarsch's Relational Epistemic Actions give us a syntax that
is perhaps more natural than the one Baltag and Moss define.
Baltag and Moss are note that it is non-standard to use the structures from
Kripke semantics inside a syntax.
In the context of interpreting and making sensible updates, however, it is
perhaps less useful to have abstraction at the cost of useful interpretations,
suggesting that Action Models are a more interesting way to model epistemic
updates.
Indeed, it would appear that at the moment Action Models are experiencing more
interest compared to Relational Epistemic Actions, as we explore in the next
section.

\subsection{Extensions of Action Models}
Action Models, when first introduced, were a concept that could be used to make
formal reasonings about actions in a similar way to Kripke semantics.
Progress in this area has added a syntax that uses Action Models as syntactic
objects, improved the expressiveness of Action Models and investigated the
synthesis of Action Models.
Much of the discussion has showed how powerful Action Models are as a way for a
third party to reason about an informational update, and improved what kind of
informational updates we can capture.\\
\\
Baltag and Moss define a powerful syntax for constructing epistemic programs.
Their syntax is as powerful as van Ditsmarsch's in Relational Epistemic Actions,
being able to express sequences of epistemic actions or make a non-deterministic
choice between two established action models.
The only objection to their syntax, as raised earlier, is the curious use of
the semantic structures within a syntax.
It is also notable that the operations of composition and non-deterministic
choice can be as costly as Action Model execution, generating $N \times M$
possible actions, given two Action Models of size $N$ and $M$
respectively.\citep{baltag2005programs}\\
\\
van Benthem, van Eijck and Kooi extend Action Models for a new language to
handle communication and change.
Their main contribution here is to improve Action Model languages with notions
of group knowledge and iterations of an action model's execution.
This logic of communication and change uses previous logics of change as a basis
and proceeds to interpret it in an epistemic fashion.\citep{benthem2006lcc}\\
\\
In a recent contribution, Hales describes an algorithm that can synthesise arbitrary
Action Models.
Thus, if an Action Model exists that can take one model of modal logic into
another, this algorithm will be able to synthesise it.
This contribution is especially notable due to it generalising outside of
epistemic logic, to any modal logic system.
Thus it is possible to transform any modal logic model into another modal logic
model using Action Models.\citep{hales13synthesis}\\
\\
This work shows how, for any system of logic, we can transform one model into
another, notwithstanding some constraints upon what transformations are
possible.
The state of the art in Action Models and this section of Dynamic Epistemic
Logic is thus focused upon improving descriptions of information updates.
None of this research discusses how an Action Model can be implemented in a
real-world system.
Instead, Action Models have become better at describing changes, but how they
should be implemented is still unclear.

\subsubsection{Possible Future Work}
Whilst the expressiveness and operators in languages involving Action Models are
improving (as in \citep{benthem2006lcc}) Action Models are treated as purely
descriptive objects.
We note two avenues of research that, as of yet, do not appear to have been
investigated.
\begin{enumerate}
	\item Action Models are external descriptions in their nature; they are only used as
	descriptions, so how an agent who is participating in the communications
	should use them is unclear
	\item Action Models have not been translated into real-world systems and no
	known method exists to change an Action Model into an implemented sequence of
	messages. The actual cost of Action Model execution in a real-world system has
	never been measured due to no-one ever implementing an Action Model.
\end{enumerate}
Thus, Action Models only describe changes, and the execution and expense of
their execution is not taken into account.
This makes it less clear how useful an Action Model would be, without a method
of transforming a description into an implementation.\\
\\
Perhaps the closest related work regarding the expense of an update might be
Bollig et. al's findings regarding communications and message passing with
dynamic logics.
The dynamic logics they utilise in their work are the same as those that van
Benthem et. al base their own work upon.
Their findings show that message passing sequences need to be of exponential
length in order to capture logical statements. \citep{bollig07mps}\\
\\
However, Bollig et. al approach message passing from the perspective of dynamic
logic.
Although van Benthem et. al use the same dynamic logic in their system, the
programs and information updates that they aim to capture might be more specific
than the general cases Bollig et. al appear to be interested in.
Thus, even though it is for a message passing sequence, there may be
structures of Action Models specifically that we can capture.\\
\\
As it stands, Bollig et. al's research is tangential to the work of van Benthem
et. al.
With regards to Action Models, the work thus far is confined to being only
useful for expressing specification of change, and whether or not such
specification implemented remains to be seen.\\
\\
van Benthem et. al also note that they had not interested themselves with methods of
generating Action Models from an ``atomic" set of Action Models. \citep{benthem2006lcc}
They lay out their intuitions about how to generate Action Models and the class
of epistemic updates, but decide not to investigate this avenue of enquiry.
This presents another unexplored avenue regarding Action Models' generation from
an atomic set of Action Models, as well as what these atomic sets are.\\
\\
Thus, although Action Models are appropriate for descriptions there is not
method to take an Action Model and transform it into an implementation.
In addition to the two areas of further investigation that we noted at the
beginning of this section, we now observe a third area for further work.
That is, it is unclear how we could generate all Action Models from a
set of atomic Action Models.

\section{Conclusion} \label{conc}
We have arrived at being able to describe the changes and updates in our game of
heads and tails by employing frameworks of logic.
We have begun at the beginning of modal logic, motivating the use of epistemic
logic and logics of changing information.
In the process, we have examined different epistemic updates, from public
announcements to more general updates like private announcements and agents
becoming suspicious.
We have come to the forefront of dynamic epistemic logic--- the
power to formally describe change in modal logic systems.\\
\\
We have also explored some of the gaps and possible avenues that are now open.
We noted three issues that we believe have not been investigated by current
research.
Firstly, current literature still does not speak much of how to translate Action Models
into implementable, practical updates.
Secondly, how Action Models can be generated is unclear, especially with regards to what
the basic ``atoms" for generation are, as well as what operations are necessary
to generate any Action Model.
Finally, Action Models are descriptive in nature, and are only useful for an
external party.
The process for an agent involved in the change to utilise an Action Model to
execute an informative update is also unclear.\\
\\
In order to move towards realising Action Model execution in a real-world,
applied system, the issues surrounding the expense of execution should be resolved.
Generating all Action Models from ``atomic" models, and how Action Models can be
decomposed into these atoms should also be investigated.

\bibliographystyle{plainnat}
\bibliography{litReview}

\end{document}
